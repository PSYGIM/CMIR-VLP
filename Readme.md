# Introduction:

This is Cross-Modality Interaction Reasoning for Enhancing Vision-Language Pre-training in Image-Text Retrieval, source code of CMIR-VLP. The paper is submited at Applied Intelligence. During the paper review stage, We provide training and validation code based on CMIR-BLIP on the Flickr30K dataset. If you are interested in other baselines in the paper, you can extract the corresponding features using large models yourself, or contact us for related support!
![image](https://github.com/PSYGIM/CMIR-VLP/blob/main/Framework%20Overvuew.jpg)

# Requirements and Installation:

We recommended the following dependencies.
Python 3.7
PyTorch 1.9,0
NumPy (>1.19.5)
The specific required in [environment.txt](https://github.com/PSYGIM/CMIR-VLP/blob/main/environment.txt).

# Download data and pre-trained model:

You can download the dataset through Google Drive. Download links are [Flickr30K](https://drive.google.com/drive/folders/1TMUpw65OWsRoH4ZupPTGyRHSTVV4DzBr?usp=sharing).

The pre-trained transformer model can be downloaded from this link:[transformer.pth](https://drive.google.com/file/d/1-_iDFfNA1f8c3NJ6hLblnUCfjn6CVCP3/view?usp=sharing)